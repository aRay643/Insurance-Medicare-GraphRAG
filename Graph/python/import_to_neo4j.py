"""
JSON ä¸‰å…ƒç»„æ•°æ®å¯¼å…¥ Neo4j æ•°æ®åº“è„šæœ¬
===================================
å°† medicineã€Insuranceã€NursingHome ä¸‰ä¸ªç›®å½•ä¸‹çš„ JSON ä¸‰å…ƒç»„æ•°æ®
å¯¼å…¥åˆ° Neo4j å›¾æ•°æ®åº“ä¸­ï¼Œå®ç°è·¨åŸŸè‡ªç„¶å…³è”å’Œæ•°æ®å»é‡ã€‚
"""

import json
import os
import re
from collections import defaultdict
from neo4j import GraphDatabase
from tqdm import tqdm

# ============================================================
# Neo4j è¿æ¥é…ç½®
# ============================================================
NEO4J_URI = "bolt://127.0.0.1:7687"
NEO4J_USER = "neo4j"
NEO4J_PASSWORD = "88888888"
NEO4J_DATABASE = "neo4j"  # Community Edition ä»…æ”¯æŒé»˜è®¤æ•°æ®åº“

# ============================================================
# æ•°æ®ç›®å½•é…ç½®
# ============================================================
# ============================================================
# æ•°æ®ç›®å½•é…ç½®
# ============================================================
BASE_DIR = r"d:\Edge_Download\Data"
DATA_SOURCES = {
    "medicine": os.path.join(BASE_DIR, "medicine"),
    "insurance": os.path.join(BASE_DIR, "Insurance"),
    "nursing_home": os.path.join(BASE_DIR, "NursingHome"),
}
SEED_DIR = os.path.join(BASE_DIR, "Seeds")

# æ‰¹é‡æäº¤å¤§å°
BATCH_SIZE = 500


def normalize_text(text):
    """
    æ ‡å‡†åŒ–æ–‡æœ¬ï¼šå»é™¤å¤šä½™ç©ºç™½ã€æ¢è¡Œç¬¦ç­‰ï¼Œç”¨äºå»é‡æ¯”è¾ƒã€‚
    """
    if not isinstance(text, str):
        return text
    # å»é™¤ \n \r å¹¶å‹ç¼©ç©ºæ ¼
    text = re.sub(r'[\r\n]+', '', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()


def normalize_name(name):
    """
    æ ‡å‡†åŒ–å®ä½“åç§°ç”¨äºå»é‡ï¼šå»é™¤æ¢è¡Œã€å¤šä½™ç©ºæ ¼ã€‚
    """
    if not isinstance(name, str):
        return str(name)
    name = re.sub(r'[\r\n]+', '', name)
    name = re.sub(r'\s+', '', name)
    return name.strip()


def clean_properties(props):
    """
    æ¸…ç†å±æ€§å€¼ï¼Œç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯ Neo4j å…¼å®¹çš„åŸºæœ¬ç±»å‹ã€‚
    - åµŒå¥—å­—å…¸ä¼šè¢«æ‰å¹³åŒ–ï¼ˆé”®åç”¨ä¸‹åˆ’çº¿è¿æ¥ï¼‰
    - åˆ—è¡¨ä¸­çš„éåŸºæœ¬ç±»å‹è½¬ä¸º JSON å­—ç¬¦ä¸²
    - æ— æ³•å¤„ç†çš„ç±»å‹è½¬ä¸º JSON å­—ç¬¦ä¸²
    """
    cleaned = {}

    def flatten(d, prefix=''):
        for k, v in d.items():
            key = f"{prefix}{k}" if not prefix else f"{prefix}_{k}"
            if isinstance(v, dict):
                flatten(v, key)
            elif isinstance(v, list):
                # Neo4j æ”¯æŒåŸºæœ¬ç±»å‹çš„æ•°ç»„
                if all(isinstance(item, (str, int, float, bool)) for item in v):
                    cleaned[key] = v
                elif all(isinstance(item, str) for item in v):
                    cleaned[key] = v
                else:
                    # åŒ…å«å¤æ‚ç±»å‹çš„åˆ—è¡¨ï¼Œè½¬ä¸º JSON å­—ç¬¦ä¸²
                    cleaned[key] = json.dumps(v, ensure_ascii=False)
            elif isinstance(v, str):
                cleaned[key] = normalize_text(v)
            elif isinstance(v, (int, float, bool)):
                cleaned[key] = v
            elif v is None:
                pass  # è·³è¿‡ None
            else:
                cleaned[key] = str(v)

    flatten(props)
    return cleaned


def load_json_files(directory):
    """
    åŠ è½½ç›®å½•ä¸‹æ‰€æœ‰ JSON æ–‡ä»¶ï¼Œè¿”å›ä¸‰å…ƒç»„åˆ—è¡¨ã€‚
    """
    triplets = []
    if not os.path.exists(directory):
        return triplets
        
    json_files = sorted([f for f in os.listdir(directory) if f.endswith('.json')])

    for filename in json_files:
        filepath = os.path.join(directory, filename)
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            if isinstance(data, list):
                for item in data:
                    item['_source_file'] = filename
                    triplets.append(item)
            elif isinstance(data, dict):
                data['_source_file'] = filename
                triplets.append(data)
        except (json.JSONDecodeError, UnicodeDecodeError) as e:
            print(f"  âš  è·³è¿‡æ–‡ä»¶ {filename}: {e}")

    return triplets


def load_seeds():
    """
    åŠ è½½ç§å­æ•°æ®
    """
    seeds = []
    if not os.path.exists(SEED_DIR):
        return seeds
        
    json_files = sorted([f for f in os.listdir(SEED_DIR) if f.endswith('.json')])
    for filename in json_files:
        filepath = os.path.join(SEED_DIR, filename)
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            if isinstance(data, list):
                seeds.extend(data)
        except Exception as e:
            print(f"  âš  è·³è¿‡ç§å­æ–‡ä»¶ {filename}: {e}")
    return seeds


def deduplicate_triplets(triplets):
    """
    å¯¹ä¸‰å…ƒç»„è¿›è¡Œå»é‡ï¼š
    åŸºäº (normalized_subject, subject_type, predicate, normalized_object, object_type) å»é‡ã€‚
    å¦‚æœæœ‰é‡å¤ï¼Œä¿ç•™å±æ€§æœ€ä¸°å¯Œçš„é‚£æ¡ã€‚
    """
    seen = {}
    for t in triplets:
        subj = normalize_name(t.get('subject', ''))
        obj = normalize_name(t.get('object', ''))
        key = (
            subj,
            t.get('subject_type', ''),
            t.get('predicate', ''),
            obj,
            t.get('object_type', ''),
        )
        if key in seen:
            # ä¿ç•™å±æ€§æ›´å¤šçš„é‚£æ¡
            existing_props = seen[key].get('properties', {})
            new_props = t.get('properties', {})
            if len(new_props) > len(existing_props):
                seen[key] = t
        else:
            seen[key] = t

    return list(seen.values())


def create_constraints_and_indexes(session):
    """
    ä¸ºæ‰€æœ‰èŠ‚ç‚¹æ ‡ç­¾åˆ›å»ºå”¯ä¸€æ€§çº¦æŸæˆ–ç´¢å¼•ã€‚
    ä½¿ç”¨ name + label ç»„åˆè¿›è¡Œå»é‡ã€‚
    """
    labels = [
        "Product", "Medical", "Brand", "Company", "Insurance",
        "Benefit", "Condition", "Exclusion", "Eligibility",
        "Org", "District", "Province",
        "ProductCategory", "Service"  # V6 æ–°å¢æ ‡ç­¾
    ]

    for label in labels:
        try:
            # å°è¯•åˆ›å»ºç´¢å¼•ï¼ˆå…¼å®¹ä¸åŒ Neo4j ç‰ˆæœ¬ï¼‰
            session.run(
                f"CREATE INDEX idx_{label.lower()}_name IF NOT EXISTS "
                f"FOR (n:{label}) ON (n.name)"
            )
            print(f"  âœ“ ç´¢å¼• idx_{label.lower()}_name å·²åˆ›å»º/å·²å­˜åœ¨")
        except Exception as e:
            print(f"  âš  åˆ›å»ºç´¢å¼• {label} æ—¶: {e}")


def import_seeds(tx, seeds):
    """
    å¯¼å…¥ç§å­æ•°æ®
    """
    print(f"  æ­£åœ¨å¯¼å…¥ {len(seeds)} æ¡ç§å­æ•°æ®...")
    for seed in seeds:
        label = seed.get('type', 'Concept')
        name = normalize_name(seed.get('name', ''))
        if not name:
            continue
            
        props = clean_properties({k:v for k,v in seed.items() if k not in ['type', 'name']})
        props['is_seed'] = True
        
        # åŠ¨æ€æ„å»º Cypher
        # æ³¨æ„ï¼šä½¿ç”¨ seed ä¸­çš„å±æ€§æ›´æ–°èŠ‚ç‚¹
        cypher = (
            f"MERGE (n:`{label}` {{name: $name}}) "
            f"SET n += $props"
        )
        tx.run(cypher, name=name, props=props)


def import_batch(tx, batch, domain):
    """
    æ‰¹é‡å¯¼å…¥ä¸‰å…ƒç»„åˆ° Neo4jã€‚
    ä½¿ç”¨ UNWIND + MERGE å®ç°é«˜æ•ˆå»é‡å¯¼å…¥ã€‚
    """
    # å‡†å¤‡æ‰¹é‡æ•°æ®
    records = []
    for t in batch:
        subj_name = normalize_name(t.get('subject', ''))
        obj_name = normalize_name(t.get('object', ''))
        subj_type = t.get('subject_type', 'Entity')
        obj_type = t.get('object_type', 'Entity')
        predicate = t.get('predicate', 'RELATED_TO')
        props = clean_properties(t.get('properties', {}))
        
        # V6 æ”¹è¿›ï¼šå†™å…¥ source_domain
        props['source_domain'] = domain
        source_file = t.get('_source_file', '')
        props['source_file'] = source_file

        records.append({
            'subj_name': subj_name,
            'subj_type': subj_type,
            'obj_name': obj_name,
            'obj_type': obj_type,
            'predicate': predicate,
            'props': props,
        })

    # æŒ‰ (subject_type, object_type, predicate) åˆ†ç»„å¯¼å…¥
    # å› ä¸º Cypher ä¸­èŠ‚ç‚¹æ ‡ç­¾å’Œå…³ç³»ç±»å‹ä¸èƒ½å‚æ•°åŒ–ï¼Œéœ€è¦åŠ¨æ€æ„å»º
    groups = defaultdict(list)
    for r in records:
        key = (r['subj_type'], r['obj_type'], r['predicate'])
        groups[key].append(r)

    for (subj_type, obj_type, predicate), group_records in groups.items():
        # å®‰å…¨æ£€æŸ¥æ ‡ç­¾åï¼ˆé˜²æ­¢æ³¨å…¥ï¼‰
        subj_type = re.sub(r'[^a-zA-Z0-9_\u4e00-\u9fff]', '_', subj_type)
        obj_type = re.sub(r'[^a-zA-Z0-9_\u4e00-\u9fff]', '_', obj_type)
        predicate = re.sub(r'[^a-zA-Z0-9_]', '_', predicate)

        params = [{'sn': r['subj_name'], 'on': r['obj_name'], 'props': r['props']}
                  for r in group_records]

        cypher = (
            f"UNWIND $params AS p "
            f"MERGE (s:`{subj_type}` {{name: p.sn}}) "
            # V6 æ”¹è¿›ï¼šMERGE æ—¶è®¾ç½® source_domain
            f"ON CREATE SET s.source_domain = p.props.source_domain " 
            f"MERGE (o:`{obj_type}` {{name: p.on}}) "
            f"ON CREATE SET o.source_domain = p.props.source_domain "
            f"MERGE (s)-[r:`{predicate}`]->(o) "
            f"SET r += p.props"
        )

        tx.run(cypher, params=params)


def clear_database(session):
    """
    æ¸…ç©ºæ•°æ®åº“ä¸­çš„æ‰€æœ‰èŠ‚ç‚¹å’Œå…³ç³»ã€‚
    åˆ†æ‰¹åˆ é™¤ä»¥é¿å…å†…å­˜é—®é¢˜ã€‚
    """
    print("\nğŸ—‘ï¸  æ­£åœ¨æ¸…ç©ºæ•°æ®åº“...")
    while True:
        result = session.run(
            "MATCH (n) WITH n LIMIT 10000 DETACH DELETE n RETURN count(*) AS deleted"
        )
        deleted = result.single()["deleted"]
        if deleted == 0:
            break
        print(f"  å·²åˆ é™¤ {deleted} ä¸ªèŠ‚ç‚¹...")
    print("  âœ“ æ•°æ®åº“å·²æ¸…ç©º")


def verify_import(session):
    """
    éªŒè¯å¯¼å…¥ç»“æœã€‚
    """
    print("\n" + "=" * 60)
    print("ğŸ“Š å¯¼å…¥éªŒè¯æŠ¥å‘Š")
    print("=" * 60)

    # 1. èŠ‚ç‚¹ç»Ÿè®¡
    print("\nğŸ“Œ èŠ‚ç‚¹ç»Ÿè®¡ï¼š")
    result = session.run(
        "MATCH (n) RETURN labels(n)[0] AS label, count(n) AS cnt "
        "ORDER BY cnt DESC"
    )
    total_nodes = 0
    for record in result:
        cnt = record["cnt"]
        total_nodes += cnt
        print(f"  {record['label']:20s} : {cnt:,}")
    print(f"  {'æ€»è®¡':20s} : {total_nodes:,}")

    # 2. å…³ç³»ç»Ÿè®¡
    print("\nğŸ”— å…³ç³»ç»Ÿè®¡ï¼š")
    result = session.run(
        "MATCH ()-[r]->() RETURN type(r) AS rel_type, count(r) AS cnt "
        "ORDER BY cnt DESC"
    )
    total_rels = 0
    for record in result:
        cnt = record["cnt"]
        total_rels += cnt
        print(f"  {record['rel_type']:25s} : {cnt:,}")
    print(f"  {'æ€»è®¡':25s} : {total_rels:,}")

    # 3. æ•°æ®æ¥æºç»Ÿè®¡
    print("\nğŸŒ å„æ•°æ®æ¥æºå…³ç³»æ•°ï¼š")
    result = session.run(
        "MATCH ()-[r]->() RETURN r.source_domain AS domain, count(r) AS cnt "
        "ORDER BY cnt DESC"
    )
    for record in result:
        print(f"  {str(record['domain']):20s} : {record['cnt']:,}")

    # 4. è·¨åŸŸå…³è”ç»Ÿè®¡
    print("\nğŸ”„ è·¨åŸŸè‡ªç„¶å…³è”ï¼š")

    # æ£€æŸ¥ä¿é™©äº§å“ä¸è¯å“ç›®å½•ä¹‹é—´çš„å…³è”ï¼ˆé€šè¿‡ Insurance èŠ‚ç‚¹ï¼‰
    result = session.run(
        "MATCH (p:Product)-[:BELONGS_TO]->(i:Insurance) "
        "WITH i, count(p) AS product_count "
        "RETURN i.name AS catalog, product_count "
        "ORDER BY product_count DESC LIMIT 5"
    )
    records_list = list(result)
    if records_list:
        print("  è¯å“-ä¿é™©ç›®å½•å…³è”:")
        for record in records_list:
            print(f"    {record['catalog']}: {record['product_count']} ä¸ªè¯å“")

    # 5. æ•°æ®æŠ½æ ·
    print("\nğŸ“ æ•°æ®æŠ½æ ·æ£€æŸ¥ï¼š")

    # Medicine æŠ½æ ·
    result = session.run(
        "MATCH (p:Product)-[r:TREATS]->(m:Medical) "
        "RETURN p.name AS drug, m.name AS disease LIMIT 3"
    )
    records_list = list(result)
    if records_list:
        print("  è¯å“-ç–¾ç—…å…³ç³»:")
        for record in records_list:
            print(f"    {record['drug']} â†’ æ²»ç–— â†’ {record['disease']}")

    # Insurance æŠ½æ ·
    result = session.run(
        "MATCH (p:Product)-[r:COVERS]->(b:Benefit) "
        "RETURN p.name AS product, b.name AS benefit LIMIT 3"
    )
    records_list = list(result)
    if records_list:
        print("  ä¿é™©-ä¿éšœå…³ç³»:")
        for record in records_list:
            print(f"    {record['product']} â†’ è¦†ç›– â†’ {record['benefit']}")

    # NursingHome æŠ½æ ·
    result = session.run(
        "MATCH (o:Org)-[r:LOCATED_IN]->(d:District) "
        "RETURN o.name AS org, d.name AS district, r.bed_count AS beds LIMIT 3"
    )
    records_list = list(result)
    if records_list:
        print("  å…»è€æœºæ„-åŒºåŸŸå…³ç³»:")
        for record in records_list:
            beds = record['beds'] if record['beds'] else 'æœªçŸ¥'
            print(f"    {record['org']} â†’ ä½äº â†’ {record['district']} (åºŠä½: {beds})")

    print("\n" + "=" * 60)
    print(f"âœ… å¯¼å…¥å®Œæˆï¼å…± {total_nodes:,} ä¸ªèŠ‚ç‚¹ï¼Œ{total_rels:,} æ¡å…³ç³»")
    print("=" * 60)


def main():
    print("=" * 60)
    print("ğŸš€ JSON ä¸‰å…ƒç»„ â†’ Neo4j å¯¼å…¥å·¥å…·")
    print("=" * 60)
    print(f"  Neo4j URI:  {NEO4J_URI}")
    print(f"  æ•°æ®åº“:     {NEO4J_DATABASE}")
    print(f"  æ•°æ®ç›®å½•:   {BASE_DIR}")
    print()

    # 1. åŠ è½½æ‰€æœ‰æ•°æ®
    all_triplets = {}
    total_raw = 0
    
    # 1b. åŠ è½½ç§å­æ•°æ®
    print("ğŸŒ± åŠ è½½ç§å­æ•°æ®...")
    seeds = load_seeds()
    print(f"  ç§å­æ•°æ®:   {len(seeds):,} æ¡")
    
    for domain, directory in DATA_SOURCES.items():
        print(f"ğŸ“‚ åŠ è½½ {domain} æ•°æ®...")
        triplets = load_json_files(directory)
        print(f"  åŸå§‹ä¸‰å…ƒç»„: {len(triplets):,} æ¡")
        deduped = deduplicate_triplets(triplets)
        print(f"  å»é‡å:     {len(deduped):,} æ¡ (å»é™¤ {len(triplets) - len(deduped)} æ¡é‡å¤)")
        all_triplets[domain] = deduped
        total_raw += len(triplets)

    total_deduped = sum(len(v) for v in all_triplets.values())
    print(f"\nğŸ“Š æ€»è®¡: {len(seeds):,} æ¡ç§å­ + {total_raw:,} æ¡åŸå§‹ â†’ {total_deduped:,} æ¡å»é‡å")

    # 2. è¿æ¥ Neo4j
    print(f"\nğŸ”Œ è¿æ¥ Neo4j ({NEO4J_URI})...")
    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))

    try:
        # éªŒè¯è¿æ¥
        driver.verify_connectivity()
        print("  âœ“ è¿æ¥æˆåŠŸ")

        with driver.session(database=NEO4J_DATABASE) as session:
            # 3. æ¸…ç©ºæ•°æ®åº“
            clear_database(session)

            # 4. åˆ›å»ºç´¢å¼•
            print("\nğŸ“‡ åˆ›å»ºç´¢å¼•...")
            create_constraints_and_indexes(session)
            
            # 5a. å¯¼å…¥ç§å­æ•°æ®
            if seeds:
                print("\nğŸŒ± å¯¼å…¥ç§å­æ•°æ®...")
                session.execute_write(import_seeds, seeds)

            # 5b. å¯¼å…¥æ•°æ®
            print("\nğŸ“¥ å¼€å§‹å¯¼å…¥æ•°æ®...")
            for domain, triplets in all_triplets.items():
                print(f"\n  ğŸ”„ å¯¼å…¥ {domain} ({len(triplets):,} æ¡)...")
                # åˆ†æ‰¹å¯¼å…¥
                for i in tqdm(range(0, len(triplets), BATCH_SIZE),
                              desc=f"  {domain}",
                              unit="batch"):
                    batch = triplets[i:i + BATCH_SIZE]
                    session.execute_write(import_batch, batch, domain)

            # 6. éªŒè¯
            verify_import(session)

    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    finally:
        driver.close()
        print("\nğŸ”Œ è¿æ¥å·²å…³é—­")


if __name__ == "__main__":
    main()
